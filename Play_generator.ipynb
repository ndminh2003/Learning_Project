{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Play_generator.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMzz8pLE57q8xWqG5Ly4uZ5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ndminh2003/Project/blob/main/Play_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eoQYPW3ZiG4R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d41a0c1-58d9-4c99-d2cc-b9b3344ca38d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip8hgJJSjqlV",
        "outputId": "2ebd80c8-2a8c-4f0f-acb1-163baff9b6e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xodA3nhGjtCj",
        "outputId": "41e1d297-3696-4944-96fa-68a835fdcbbe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6EiyFNKkNuT",
        "outputId": "b926f215-70da-49b7-996c-9cff8f4ee00c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "metadata": {
        "id": "pfZnrkMBkQMW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets look at how part of our text is encoded\n",
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avDHw3sVkXlN",
        "outputId": "e729a532-8d9f-4067-b139-de981ca6847f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zescLvL9kkFs",
        "outputId": "08e92827-ecb7-4152-c587-bdb94b14c0c8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100  # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "metadata": {
        "id": "3TNobaFJknXC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "AT1hg61lkiEk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
      ],
      "metadata": {
        "id": "HFEB5Jbzk72D"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STxi-euuk_FJ",
        "outputId": "55092557-fb19-46d4-db88-c18752f5e0fa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "taLlxKAnlJ71"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qCSaRralPda",
        "outputId": "40855415-5b6c-4bce-8f8f-cd19324a581d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sYun333lUxy",
        "outputId": "a040131d-02aa-4e15-c193-47263430df02"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9StP5XQm7O-",
        "outputId": "d41775fd-877d-451c-d9f2-c97d18e91e38"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[ 3.59176099e-03 -3.93945165e-03 -5.45367086e-03 ... -1.99474016e-04\n",
            "   -4.66785859e-03 -5.63229714e-03]\n",
            "  [ 3.01952381e-03 -2.49583181e-03 -5.66403195e-03 ... -7.91824656e-04\n",
            "    1.65535801e-03 -5.07866545e-03]\n",
            "  [-9.55192000e-03 -4.84814867e-03 -6.26328820e-03 ... -6.04148896e-04\n",
            "    3.78282368e-03 -6.31288392e-04]\n",
            "  ...\n",
            "  [-6.94018789e-04  5.80838881e-04 -1.64791383e-02 ...  1.18949367e-02\n",
            "   -8.64649098e-03  8.19950085e-03]\n",
            "  [-1.81797892e-04  1.96943828e-03 -1.68362930e-02 ...  5.14874235e-03\n",
            "   -5.27779199e-03  1.11209797e-02]\n",
            "  [-5.69483964e-03 -1.44291669e-04 -1.17729586e-02 ...  6.44039654e-04\n",
            "   -4.86962125e-03  4.65851557e-03]]\n",
            "\n",
            " [[-5.23264345e-04  2.84367264e-03  1.45340024e-03 ...  1.25428347e-03\n",
            "    2.26206798e-03  3.78425000e-04]\n",
            "  [ 3.04812519e-03  9.41042847e-04 -7.42238015e-03 ...  8.28298042e-04\n",
            "   -3.22986254e-03  5.21536870e-03]\n",
            "  [ 5.91000263e-03 -6.65985700e-03 -8.23313929e-03 ... -1.03315525e-03\n",
            "    4.60711727e-03  3.19570932e-03]\n",
            "  ...\n",
            "  [ 5.61681949e-03  1.48863692e-04 -6.82228478e-03 ...  7.95001397e-05\n",
            "   -3.09222401e-03 -4.37078672e-03]\n",
            "  [ 2.36450229e-03  2.87226634e-04 -1.39377918e-03 ... -4.91095427e-03\n",
            "   -6.57880912e-03 -7.88337551e-03]\n",
            "  [-5.41219604e-04  4.10781661e-03 -2.10214779e-03 ... -8.91648419e-03\n",
            "   -7.55001139e-03 -1.29695619e-02]]\n",
            "\n",
            " [[-2.18496751e-03 -9.75470757e-04  1.50578539e-03 ...  3.60606937e-03\n",
            "   -2.01387797e-03  1.75817101e-03]\n",
            "  [-1.27995089e-02 -5.33229299e-03 -1.25985756e-03 ...  2.85570254e-03\n",
            "    1.09512173e-03  3.50087392e-03]\n",
            "  [-1.58691071e-02 -5.74879767e-03 -4.00936976e-03 ...  1.96134299e-03\n",
            "   -5.31207770e-03  9.76806390e-04]\n",
            "  ...\n",
            "  [-7.12225214e-03 -1.10465959e-02 -5.67284413e-03 ... -2.21246248e-03\n",
            "   -8.74854997e-03  5.99100208e-03]\n",
            "  [-5.98067883e-03 -1.20202648e-02 -3.14470613e-03 ...  1.71584240e-03\n",
            "   -6.41059550e-03  2.96046608e-03]\n",
            "  [-9.56873316e-03 -1.16802705e-02  1.11590419e-03 ...  5.93822822e-03\n",
            "   -1.40017481e-03  9.15377028e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 4.89600375e-03  2.52702041e-03 -1.50451402e-03 ...  1.08037551e-03\n",
            "    2.28834193e-04  2.05745595e-03]\n",
            "  [ 1.18213259e-02  1.09473430e-03  3.21960542e-05 ...  9.53980896e-04\n",
            "   -5.15673496e-03  3.17331753e-03]\n",
            "  [ 7.13977590e-03  9.81722958e-04  3.31023755e-03 ... -4.18486772e-03\n",
            "   -7.81827606e-03 -1.63000589e-03]\n",
            "  ...\n",
            "  [-4.36806353e-03  4.10307199e-04 -1.20921788e-04 ... -8.08847696e-03\n",
            "    6.46676915e-03 -9.42042470e-03]\n",
            "  [ 1.90743245e-04 -2.81054527e-05 -9.22818761e-03 ... -6.58681849e-03\n",
            "   -7.31430890e-04 -1.32323301e-03]\n",
            "  [-5.69773931e-03 -1.78029784e-03 -4.18208819e-03 ... -3.71421454e-04\n",
            "    3.06826644e-03  6.02722447e-03]]\n",
            "\n",
            " [[ 3.12353368e-04 -3.44281434e-04 -1.59448874e-03 ... -1.14166678e-03\n",
            "    5.38091548e-03 -2.15188041e-03]\n",
            "  [-5.64660504e-03 -4.68352810e-04 -2.63955747e-03 ... -1.24200084e-03\n",
            "    4.94478329e-04  2.95521895e-04]\n",
            "  [-8.35784012e-05 -8.78259074e-04 -9.54703148e-03 ... -1.09828869e-03\n",
            "   -5.03841555e-03  3.58510087e-03]\n",
            "  ...\n",
            "  [ 2.46527861e-03  1.23562128e-03 -1.08438358e-02 ...  8.02459894e-04\n",
            "   -6.56863209e-03  2.06329743e-03]\n",
            "  [ 1.36594730e-03 -5.76918479e-03 -8.65297392e-03 ... -2.95018847e-03\n",
            "   -4.94831335e-03  1.62116077e-03]\n",
            "  [ 7.03096506e-04 -4.10437072e-03 -8.13374948e-03 ... -3.82122723e-03\n",
            "    1.68660632e-03 -1.06240775e-04]]\n",
            "\n",
            " [[-5.58079779e-03 -2.00528256e-03  3.86946369e-03 ...  4.99975448e-03\n",
            "    3.12020024e-03  6.39238674e-03]\n",
            "  [-1.14324829e-02 -3.67343985e-03  9.15835146e-04 ...  2.49977643e-03\n",
            "   -3.16636125e-03  4.19503730e-03]\n",
            "  [-4.16979333e-03 -4.11831634e-03 -6.72384771e-03 ...  1.30676839e-03\n",
            "   -8.12505279e-03  8.76588840e-03]\n",
            "  ...\n",
            "  [-8.73517618e-03  2.83386884e-03 -1.45497564e-02 ...  2.35924800e-03\n",
            "    5.59406402e-03  1.08186901e-02]\n",
            "  [-6.58341357e-03  8.59087892e-03 -1.16416067e-02 ...  4.53225244e-03\n",
            "    8.80566426e-03  8.14181287e-03]\n",
            "  [-9.46103409e-03 -4.54581808e-04 -8.54743365e-03 ...  1.38731080e-03\n",
            "    5.97962365e-03  9.78291035e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdyXg6MYnC21",
        "outputId": "d6f36af1-1783-4e7f-8f72-7ba256ae2b31"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[ 0.00359176 -0.00393945 -0.00545367 ... -0.00019947 -0.00466786\n",
            "  -0.0056323 ]\n",
            " [ 0.00301952 -0.00249583 -0.00566403 ... -0.00079182  0.00165536\n",
            "  -0.00507867]\n",
            " [-0.00955192 -0.00484815 -0.00626329 ... -0.00060415  0.00378282\n",
            "  -0.00063129]\n",
            " ...\n",
            " [-0.00069402  0.00058084 -0.01647914 ...  0.01189494 -0.00864649\n",
            "   0.0081995 ]\n",
            " [-0.0001818   0.00196944 -0.01683629 ...  0.00514874 -0.00527779\n",
            "   0.01112098]\n",
            " [-0.00569484 -0.00014429 -0.01177296 ...  0.00064404 -0.00486962\n",
            "   0.00465852]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and finally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# and of course its 65 values representing the probabillity of each character occuring next"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXLftGIunIqU",
        "outputId": "c535b2cf-e978-4f80-e9db-1c554c84d5bc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[ 3.5917610e-03 -3.9394516e-03 -5.4536709e-03  2.5459931e-03\n",
            "  1.1389293e-03  9.0083934e-04  1.7996847e-03  3.2723867e-03\n",
            " -1.0929516e-03  8.4780436e-03 -6.4213006e-03 -5.8094510e-03\n",
            " -6.8529295e-03  4.2464910e-04  6.9220504e-03  2.3133776e-03\n",
            " -3.8667661e-03 -2.8500024e-03 -4.2664178e-05 -1.6553054e-04\n",
            "  2.2476586e-03  1.0900612e-03  3.0927227e-03 -1.6313645e-03\n",
            "  2.2545638e-03 -2.6969456e-03  3.4609244e-03  2.9292514e-03\n",
            "  1.8381404e-03  5.8824965e-03  5.7447224e-04 -2.0663161e-04\n",
            "  4.8485808e-03  1.5003586e-03 -5.6997095e-03 -4.1681868e-03\n",
            " -2.3847972e-03  4.1243127e-03  2.1424918e-03  4.3225545e-03\n",
            " -3.0105547e-03  1.7251174e-03 -3.5220417e-03  3.0252775e-03\n",
            "  3.6914444e-03 -2.8800196e-03  2.7425098e-03 -7.8142136e-03\n",
            " -5.2357715e-04  1.3000509e-03  8.9242146e-04 -5.9074984e-04\n",
            " -4.6104491e-03 -9.0821262e-04  3.7879976e-03  3.3898564e-04\n",
            "  3.6420645e-03  8.8667357e-04 -4.1858614e-03 -8.2726423e-03\n",
            " -1.9657076e-04  1.7001823e-04 -1.9947402e-04 -4.6678586e-03\n",
            " -5.6322971e-03], shape=(65,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uTUAlJvBnO4T",
        "outputId": "35f19e23-f654-4b3f-ed44-a7a4fb5c0e17"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"'LM hWPLNa ?N,k?ZJwwdqQCrwKx '3p.Px'vpCzO;TLOViCVFZ tZ?avtCvbtA:'?US mZjUIDxP.;e&eW\\nO3R:eNgAVBOYb,gZ\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "metadata": {
        "id": "Fjy_HaK3nTWz"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "5yg49I8HnXG0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "APUAsC8OnZXP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(data, epochs=50, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W__sfIXHnc-l",
        "outputId": "16d97c8a-09d4-41d2-dfb4-ab43bdd383e7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "172/172 [==============================] - 14s 64ms/step - loss: 2.5659\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 12s 65ms/step - loss: 1.8646\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 13s 66ms/step - loss: 1.6217\n",
            "Epoch 4/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 1.4926\n",
            "Epoch 5/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.4139\n",
            "Epoch 6/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.3586\n",
            "Epoch 7/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 1.3140\n",
            "Epoch 8/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.2751\n",
            "Epoch 9/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 1.2388\n",
            "Epoch 10/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.2040\n",
            "Epoch 11/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.1680\n",
            "Epoch 12/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 1.1321\n",
            "Epoch 13/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.0945\n",
            "Epoch 14/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.0557\n",
            "Epoch 15/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.0163\n",
            "Epoch 16/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.9771\n",
            "Epoch 17/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.9358\n",
            "Epoch 18/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.8965\n",
            "Epoch 19/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.8576\n",
            "Epoch 20/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 0.8185\n",
            "Epoch 21/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.7832\n",
            "Epoch 22/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.7495\n",
            "Epoch 23/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.7203\n",
            "Epoch 24/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.6905\n",
            "Epoch 25/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.6631\n",
            "Epoch 26/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.6412\n",
            "Epoch 27/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.6193\n",
            "Epoch 28/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.5998\n",
            "Epoch 29/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.5826\n",
            "Epoch 30/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.5665\n",
            "Epoch 31/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.5513\n",
            "Epoch 32/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 0.5391\n",
            "Epoch 33/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.5288\n",
            "Epoch 34/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.5174\n",
            "Epoch 35/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.5047\n",
            "Epoch 36/50\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 0.4972\n",
            "Epoch 37/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.4878\n",
            "Epoch 38/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 0.4819\n",
            "Epoch 39/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.4737\n",
            "Epoch 40/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.4692\n",
            "Epoch 41/50\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 0.4632\n",
            "Epoch 42/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 0.4582\n",
            "Epoch 43/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.4533\n",
            "Epoch 44/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.4492\n",
            "Epoch 45/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.4471\n",
            "Epoch 46/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.4415\n",
            "Epoch 47/50\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.4391\n",
            "Epoch 48/50\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 0.4352\n",
            "Epoch 49/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 0.4327\n",
            "Epoch 50/50\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.4315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "metadata": {
        "id": "oMT-tf0HrimV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "Er-yL37-rqNS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "    \n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "6M-AtFNOrvjy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A42HfyKUr9hO",
        "outputId": "c8bc4898-630a-4efa-b99f-e4632fd43cb4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a starting string: Hello\n",
            "Hellock, believe your honours\n",
            "Prove as in armed age to age, my boots\n",
            "I' the wish despite of all the chests and roots;\n",
            "His lords have done this feather souls?\n",
            "You understand me: therefore early then will from\n",
            "them, my household does!\n",
            "\n",
            "JOHN OF GAUNT:\n",
            "Respecting both the unacking my master:\n",
            "Eng are they must confince them,\n",
            "As if I weep again:\n",
            "'Romation as my soldiers: I am\n",
            "customent with thy life, frighter thus.\n",
            "\n",
            "HASTINGS:\n",
            "I tell thee, man, sir.\n",
            "\n",
            "ABRAHAM:\n",
            "Do plufer charitation! brothers and Percius was left thee gone,\n",
            "As thou didst kill of woman ten times for the provost.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Nay Warwicks R KING RICHARD II:\n",
            "Mine ear is named for approve his gracious stars,\n",
            "And kneel by Edward cannot well despair,\n",
            "For ever I war advanced the very witness\n",
            "Whom we may do it, and would happy from the c\n"
          ]
        }
      ]
    }
  ]
}