{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np\nimport time\nimport os\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-06T00:47:29.252245Z","iopub.execute_input":"2022-11-06T00:47:29.252964Z","iopub.status.idle":"2022-11-06T00:47:32.357355Z","shell.execute_reply.started":"2022-11-06T00:47:29.252906Z","shell.execute_reply":"2022-11-06T00:47:32.355297Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# checkpoint \ncheckpoint = \"microsoft/DialoGPT-large\"\n# download and cache tokenizer\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# download and cache pre-trained model\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2022-11-06T00:59:53.535032Z","iopub.execute_input":"2022-11-06T00:59:53.535516Z","iopub.status.idle":"2022-11-06T01:02:49.516407Z","shell.execute_reply.started":"2022-11-06T00:59:53.535480Z","shell.execute_reply":"2022-11-06T01:02:49.514713Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"718a5964d7bc4d7e892df8dd1fe2d06f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30603921459b4e60950e2f87fb8a04b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c8dc8314c44466286ee4d8e4e296967"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f5b87877658495eb49a2f1e93a6cf45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9171a1c769544fdb6a11c7cb2465e25"}},"metadata":{}}]},{"cell_type":"code","source":"class DaiMinh():\n    # initialize\n    def __init__(self):\n        # once chat starts, the history will be stored for chat continuity\n        self.chat_history_ids = None\n        # make input ids global to use them anywhere within the object\n        self.bot_input_ids = None\n        # a flag to check whether to end the conversation\n        self.end_chat = False\n        # greet while starting\n        self.welcome()\n        \n    def welcome(self):\n        print(\"Initializing DaiMinh ...\")\n        # some time to get user ready\n        time.sleep(2)\n        print('Type \"bye\" or \"quit\" or \"exit\" to end chat \\n')\n        # give time to read what has been printed\n        time.sleep(3)\n        # Greet and introduce\n        greeting = np.random.choice([\n            \"Welcome, I am Minh, here to help you\",\n            \"Hey, Great day! I am Minh\",\n            \"Hello, it's my pleasure meeting you\",\n            \"Hi, I am a Minh. Let's chat!\"\n        ])\n        print(\"DaiMinh >>  \" + greeting)\n        \n    def user_input(self):\n        # receive input from user\n        text = input(\"User    >> \")\n        # end conversation if user wishes so\n        if text.lower().strip() in ['bye', 'quit', 'exit']:\n            # turn flag on \n            self.end_chat=True\n            # a closing comment\n            print('DaiMinh >>  Bye!')\n            time.sleep(1)\n            print('\\nQuitting ...')\n        else:\n            # continue chat, preprocess input text\n            # encode the new user input, add the eos_token and return a tensor in Pytorch\n            self.new_user_input_ids = tokenizer.encode(text + tokenizer.eos_token, \\\n                                                       return_tensors='pt')\n\n    def bot_response(self):\n        # append the new user input tokens to the chat history\n        # if chat has already begun\n        if self.chat_history_ids is not None:\n            self.bot_input_ids = torch.cat([self.chat_history_ids, self.new_user_input_ids], dim=-1) \n        else:\n            # if first entry, initialize bot_input_ids\n            self.bot_input_ids = self.new_user_input_ids\n        \n        # define the new chat_history_ids based on the preceding chats\n        # generated a response while limiting the total chat history to 1000 tokens, \n        self.chat_history_ids = model.generate(self.bot_input_ids, max_length=1000, \\\n                                               pad_token_id=tokenizer.eos_token_id)\n            \n        # last ouput tokens from bot\n        response = tokenizer.decode(self.chat_history_ids[:, self.bot_input_ids.shape[-1]:][0], \\\n                               skip_special_tokens=True)\n        # in case, bot fails to answer\n        if response == \"\":\n            response = self.random_response()\n        # print bot response\n        print('DaiMinh >>  '+ response)\n        \n    # in case there is no response from model\n    def random_response(self):\n        i = -1\n        response = tokenizer.decode(self.chat_history_ids[:, self.bot_input_ids.shape[i]:][0], \\\n                               skip_special_tokens=True)\n        # iterate over history backwards to find the last token\n        while response == '':\n            i = i-1\n            response = tokenizer.decode(self.chat_history_ids[:, self.bot_input_ids.shape[i]:][0], \\\n                               skip_special_tokens=True)\n        # if it is a question, answer suitably\n        if response.strip() == '?':\n            reply = np.random.choice([\"I don't know\", \n                                     \"I am not sure\",\n                                     \"No idea\",\"I will punch you if you say that again\"\n                                    ])\n        # not a question? answer suitably\n        else:\n            reply = np.random.choice([\"Great\", \n                                      \"Fine. What's up?\", \n                                      \"Okay\",\"The f**k\",\"Wanna die bruh ?\"\n                                     ])\n        return reply","metadata":{"execution":{"iopub.status.busy":"2022-11-06T01:08:24.754541Z","iopub.execute_input":"2022-11-06T01:08:24.757710Z","iopub.status.idle":"2022-11-06T01:08:24.780532Z","shell.execute_reply.started":"2022-11-06T01:08:24.757636Z","shell.execute_reply":"2022-11-06T01:08:24.779043Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# build a ChatBot object\nbot = DaiMinh()\n# start chatting\nwhile True:\n    # receive user input\n    bot.user_input()\n    # check whether to end chat\n    if bot.end_chat:\n        break\n    # output bot response\n    bot.bot_response()    ","metadata":{"execution":{"iopub.status.busy":"2022-11-06T01:08:30.127555Z","iopub.execute_input":"2022-11-06T01:08:30.127975Z","iopub.status.idle":"2022-11-06T01:11:35.010853Z","shell.execute_reply.started":"2022-11-06T01:08:30.127944Z","shell.execute_reply":"2022-11-06T01:11:35.008991Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Initializing DaiMinh ...\nType \"bye\" or \"quit\" or \"exit\" to end chat \n\nDaiMinh >>  Hey, Great day! I am Minh\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User    >>  Nice to meet you ?\n"},{"name":"stdout","text":"ChatBot >>  Nice to meet you too.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User    >>  Are you gay ?\n"},{"name":"stdout","text":"ChatBot >>  I am not.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User    >>  So are you dumb ?\n"},{"name":"stdout","text":"ChatBot >>  I am not.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User    >>  Okay so what are you ?\n"},{"name":"stdout","text":"ChatBot >>  I am a human.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User    >>  Really ?\n"},{"name":"stdout","text":"ChatBot >>  I am a human.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User    >>  What is your name ?\n"},{"name":"stdout","text":"ChatBot >>  I am a human.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User    >>  Okay you are sounding creepy\n"},{"name":"stdout","text":"ChatBot >>  I am a human.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User    >>  stop\n"},{"name":"stdout","text":"ChatBot >>  I am a human.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User    >>  what the fuck\n"},{"name":"stdout","text":"ChatBot >>  I am a human.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User    >>  you are not human\n"},{"name":"stdout","text":"ChatBot >>  I am a human.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User    >>  you are a robot\n"},{"name":"stdout","text":"ChatBot >>  I am a robot.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User    >>  let's start again\n"},{"name":"stdout","text":"ChatBot >>  I am a\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User    >>  how is the weather\n"},{"name":"stdout","text":"ChatBot >>  I am a robot.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User    >>  quit\n"},{"name":"stdout","text":"DaiMinh >>  Bye!\n\nQuitting ...\n","output_type":"stream"}]}]}